
\section{Notes}

\subsection{\emoji{light-bulb}Project topics}
\label{subscn:proj_topics}
This section briefly recalls the project's main topics, highlighting potential issues that could be addressed in future research \cite{Oren2022}.

\myrulefill{.5\linewidth}{\textit{November, 22}}{\noindent Material mapping}
\newline
Material mapping is defined in \cite{Oren2022} as a process allowing tracking of the gravel's location on-site. Indeed, it relies on the capability to distinguish the material from the surroundings. Computer vision is usually exploited to reach such a goal. In \cite{Oren2022} an initial hint from human operators is used to reach dynamic mapping during the task execution.

\myrulefill{.5\linewidth}{\textit{November, 22}}{\noindent Localization }
\newline
Localization plays a key role in multi-agent robotic applications. In \cite{Oren2022} a standard EKF approach based on wheel odometry and IMU measurements is improved by exploiting visual localization gained from a master agent (UAV) to increase the estimated position precision on the others (UGVs).

\subsection{\emoji{light-bulb}Points of interest}
\label{subscn:points_interest}

\myrulefill{.5\linewidth}{\textit{November, 23}}{\noindent Relative distances}
\newline
Considering the localization problem, a first question arises from \cite{Oren2022}. Does the visual-EKF use the images to detect only the UGV absolute position or does it also considers the relative distances between the agents? The framework I'm thinking of is the following. Consider the model notation used in \cite{Oren2022}:

\begin{subequations}
    \label{eqn:model_ss}
    \begin{align}
        x_t &= f(x_{t-1},u_t) + \omega_t, \\
        z_t &= h(x_t) + e_t,
    \end{align}
\end{subequations}

with $x\in\mathbb{R}^n$ the state vector, $u\in\mathbb{R}^m$ the control input, $z\in\mathbb{R}^m$ the measured output, and $(\omega,e)$ respectively the process and measurement noise. As I understand, \cite{Oren2022} considers the standard EKF as provided with $z_{s} = [p_{od}, v_{IMU}]$, where $p_{od}$ the position computed from the wheel odometry, and $v_{IMU}$ the velocity measured by the IMU. Instead, the visual-EKF considers an augmented measurements vector $z_v = [p_{od}, p_{img},v_{IMU}]$, where the absolute position of the UGV is also estimated from the UAV camera. 

\medskip
I was wondering if relative distances among UGVs could also be used to further improve the estimation precision, following the approach proposed in \textit{docs/FormationFly\_Journal\_v3.pdf}. The relative distances could be obtained either from the UAV camera or through additional sensors, such as UWB sensors (e.g. \cite{UWBmodules,UWBdecawave}). The following benefits could come from this approach: 

\begin{itemize}
    \item Errors on initial anchor position could be addressed 
    \item Improved estimation precision (see \textit{docs/FormationFly\_Journal\_v3.pdf}). Moreover, a point left open in such analysis is the possibility of exploiting EKF covariance matrix estimation to infer on the goodness of a relative distance measurement. Each measurement could be weighted according to the inferred estimation goodness in this case. 
\end{itemize}

\myrulefill{.5\linewidth}{\textit{November, 29}}{\noindent Adaptive observers}
\newline
An issue that could be interesting to address is the estimation of model parameters along with the state vector. I'm thinking for instance of the possibility of estimating online the mass of the UGV. By doing so there could be a direct measure of how much weight the single agent is carrying during the site preparation task. Adaptive observers could be used to test if this approach is feasible. One could think of using an EKF-based approach again. However, if the mission's cycle time and the system's computational capabilities are sufficiently long/high, an optimization-based approach could be tried, following for instance \cite{Oliva01,Oliva02}. In this regard, I have a couple of questions: 

\begin{itemize}
    \item Is there a GCS in the experiment?
    \item If so, which algorithms are centralized in the GCS and which are instead run by single agents?    
\end{itemize}

However, the benefits of this approach could be the followings:

\begin{itemize}
    \item More precise and robust estimation compared to EKF
    \item Online system diagnosis through model parameters monitoring
\end{itemize}

\myrulefill{.5\linewidth}{\textit{November, 29}}{\noindent Hybrid observer}
\label{par:hyb_obs}
\newline
We are currently working on a hybrid observer to exploit measurements with consistently different sampling times (it's still in an early stage). Consider the following setup, where the plant describes for instance a rover's dynamics: 

\begin{equation}
    \label{eqn:multirate_ss}
    \mathcal{P} \text{ : }    
    \begin{cases}
        x_t &= f(x_{t-1},u_t) + \omega_t \\
        z_t &= h(x_t) + e_t \\
        d_{t} &= g(x_{t-K-1},u_{t-K}) + v_t
    \end{cases}
\end{equation}

where $x_t\in\mathbb{R^n}$ is the state vector, and more specifically $p$ is the rover position, and $v$ the velocity. $z_t\in\mathbb{R}^p$ is a vector of IMU measurements available at a high sampling rate, namely the same rate used to integrate the system. Instead, $d_t\in\mathbb{R}^l$ is a vector of relative distances whose sampling rate is lower than the previous one, i.e., it is acquired every $K$ samples. Lastly, $\omega_t$ is the process noise, and $(e_t,v_t)$ are the measurement noise. Let's consider $||e_t|| \geq ||v_t||$ as a general assumption. Thus, low-frequency measurements are considered more reliable than high-frequency ones. In the current framework, i.e. the rover, we consider the following sensors available: 

\begin{itemize}
    \item IMU: providing acceleration at high frequency but with high noise and drifting issues.
    \item UWB: providing relative distances with respect to anchors, but with a lower noise.
\end{itemize}

\medskip
The idea is to design an observer in the following form: 

\begin{equation}
    \label{eqn:observer_hyb}
    \mathcal{O} \text{ : }
    \begin{cases}
        \xi_t &= f_o(\xi_{t-1},\theta,u_t,z_t) \\
        \theta &= j^{+}(\tilde{p}_t) \\
        \hat{x}_t &= \gamma(\xi_t,u_t) 
    \end{cases}
\end{equation}

where $\xi_t\in\mathbb{R}^o$ is the observer state, $\hat{x}_t\in\mathbb{R}^n$ is the observer output (i.e. the state estimation), and $\theta\in\mathbb{R}^h$ is a set of parameters determined by a jump-map $j^{+}$. Such map is responsible for a discrete state-reset event on $\xi_t$, and exploits information gathered from the UWB measurments. Currently, we are performing a gradient-like optimization on the rover position by using $d_t$ measurements. The main idea is depicted in \cref{fig:hyb}, where $(z_t^*,d_t^*)$ are $\mathcal{P}$ measurements without noise, and the control input $u$ is assumed zero to ease the notation. Note that an event trigger \textbf{E} is included in the scheme as the jump map $j^+$ acts on the observer state only when the low-frequency measurement $d_t$ is collected. In \cref{fig:hyb} the output of the optimization process is referred to as $\tilde{p}_t$, and it is provided to the jump map $j^+$ instead of the raw measurements $d_t$.

\medskip
The reset performed by $j^+$, aims to exploit the more reliable information from the low-frequency measurements to improve the estimation performance on $\hat{x}_t$. 

%%

\begin{figure}[h!]        
	\centering
	\begin{tikzpicture}[
			squarednode_black/.style={rectangle, draw=black!60, fill=black!5, very thick},
			circlenode/.style={circle, draw=black!60, fill=black!5, very thick},
			node distance=3cm]
			
		%Nodes
            % Plant
		\node[squarednode_black,minimum width = 2.5cm,minimum height = 1cm] (P) {$\bm{\mathcal{P}}$};
            % Observer
		\node[squarednode_black,minimum width = 2.5cm,minimum height = 1cm] (O) [xshift=-0cm,below=of P] {$f_o$};
            % jump
            \node[squarednode_black,minimum width = 2.5cm,minimum height = 1cm](J) [right=of O] {$j^+$};                     
            % S1
            \node[draw,circle,minimum size=0.5cm,inner sep=0pt] (S1) [yshift=1.5cm,below=of P] {$+$};
            % S2
            \node[draw,circle,minimum size=0.5cm,inner sep=0pt] (S2) [xshift=1cm,right=of P] {$+$};
            % Gradient
            \node[squarednode_black,minimum width = 2.5cm,minimum height = 1cm] (OPT) [yshift=2cm,below=of S2] {OPT};
		
		

		%Lines
            % Plant
		\draw[black, ultra thick, ->] (P.east) -- ([xshift=2cm]P.east) node [xshift=0cm,above] (TextNode) {$d_t^*$} -- (S2.west);
            % z meas            
            \draw[black, ultra thick, ->] (P.south) -- (S1.north) node [yshift=0.5cm,right] (TextNode) {$z^*_t$};
            \draw[black, ultra thick, ->] ([xshift=1cm]S1.east) -- (S1.east) node [xshift=0.5cm,above] (TextNode) {$e_t$};
            \draw[black, ultra thick, ->] (S1.south) -- (O.north) node [yshift=0.5cm,right] (TextNode) {$z_t$};
            % y meas            
            \draw[black, ultra thick, ->] ([yshift=1cm]S2.north) -- (S2.north) node [yshift=0.5cm,right] (TextNode) {$v_t$};
            \draw[black, ultra thick, ->] (S2.south) -- ([yshift=-0.5cm]S2.south) node [right] (TextNode) {$d_t$} -- (OPT.north);
            % theta and switch
            \draw[black, ultra thick,->] (OPT.south) -- (J.north) node [yshift=0.7cm,right] (TextNode) {$\tilde{p}_t$};
            \draw[black, ultra thick,] (J.west) -- ([xshift=1.5cm]O.east) node [xshift=0.5cm,above] (TextNode) {$\theta$};
            \draw[black, ultra thick,] ([yshift=0.5cm,xshift=1.5cm]O.east) node [xshift=0cm,above] (TextNode) {\textbf{E}} -- ([xshift=1cm]O.east) -- (O.east);
            %\draw[black, ultra thick,] (E.south) --([yshift=0.5cm,xshift=1.5cm]O.east);
            
            % xhat
            \draw[black, ultra thick, ->] (O.west) -- ([xshift=-1.5cm]O.west) node [xshift=0.7cm,above] (TextNode) {$[\hat{p}_t,\hat{v}_t]$} -- ([yshift=-1.5cm,xshift=-1.5cm]O.west) -- ([yshift=-1.5cm,xshift=9cm]O.west) --([xshift=2.2cm]OPT.center) --(OPT.east);
		
		
		
	\end{tikzpicture}	
	\caption{\label{fig:hyb}\centering Hybrid observer}
\end{figure}

\myrulefill{.5\linewidth}{\textit{January, 9}}{\noindent Hybrid observer - rover example}

The Hybrid Observer from \cref{par:hyb_obs} has been developed, and the main characteristics of the framework are detailed here. 

\begin{itemize}
    \item Plant $\mathcal{P}$: the model of the rover is the following

    \begin{equation}
        \label{eqn:P_rover}
        \mathcal{P} \ : \ 
        \begin{cases}
            \dot{x}_1 &= x_2 \\
            \dot{x}_2 &= u_x \\    
            \dot{x}_3 &= x_4 \\
            \dot{x}_4 &= u_y \\                        
        \end{cases},
    \end{equation}
    
    where $[x_1,x_2,x_3,x_4] = [p_x,v_x,p_y,v_y]$, and the dynamics are modeled as a simple double integrator. The model input on the two dimensions is $\bm{u} = [u_x,u_y] \in \mathbb{R}^2$. As far as the measurements are concerned, the following holds: 

    \begin{itemize}
        \item IMU measure: 
        \begin{equation}
            \label{eqn:IMU}
            z = 
            \begin{bmatrix}
                \dot{x_2} & \dot{x}_4
            \end{bmatrix}
            + e \in \mathbb{R}^2.
        \end{equation} 
        The IMU error model is additive and yields $e = \mathcal{N}(0,1) \ [m/s^2]$.
        \item UWB measure: consider a set of $N$ anchors, with positions $\bm{p_{a_i}} = [p_{a_i,x} \ p_{a_i,y}] \in \mathbb{R}^2$. The UWB sensors provide a set of distances 

        \begin{equation}
            \label{eqn:UWB_meas}
            \bm{d} = [d_1,\dots,d_n] + v \ \in\mathbb{R}^N,
        \end{equation}        
        with $v \in \mathcal{N}(0.07,0.2) \ [m]$ and
        \begin{equation}
            \label{eqn:UWB_dist_def}
            d_i \triangleq \sqrt{(x_1-p_{a_i,x})^2 + (x_3-p_{a_i,y})^2}.
        \end{equation}        
    \end{itemize}
\end{itemize}